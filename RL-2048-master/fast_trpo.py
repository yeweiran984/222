import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import gymnasium as gym
import game2048
import time
import tqdm
from typing import List, Tuple
from trpo_game2048_simple import SimplePolicy, SimpleValue
from training_monitor import TrainingMonitor
from itertools import count


class VectorizedEnv:
    """å‘é‡åŒ–ç¯å¢ƒåŒ…è£…å™¨ - å¹¶è¡Œè¿è¡Œå¤šä¸ªç¯å¢ƒ"""
    def __init__(self, num_envs: int = 4):
        self.num_envs = num_envs
        self.envs = [gym.make("Game2048-v0") for _ in range(num_envs)]
        
    def reset(self):
        """é‡ç½®æ‰€æœ‰ç¯å¢ƒ"""
        states = []
        for env in self.envs:
            state, _ = env.reset()
            states.append(state)
        return np.array(states)
    
    def step(self, actions):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        states = []
        rewards = []
        dones = []
        
        for i, (env, action) in enumerate(zip(self.envs, actions)):
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            if done:
                state, _ = env.reset()
            
            states.append(state)
            rewards.append(reward)
            dones.append(done)
        
        return np.array(states), np.array(rewards), np.array(dones)
    
    def close(self):
        """å…³é—­æ‰€æœ‰ç¯å¢ƒ"""
        for env in self.envs:
            env.close()


class FastTRPO:
    """ä¼˜åŒ–é‡‡æ ·é€Ÿåº¦çš„TRPO"""
    
    def __init__(self, num_envs=4, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.num_envs = num_envs
        
        self.policy = SimplePolicy().to(device)
        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=3e-4)
        self.value = SimpleValue().to(device)
        self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=3e-4)

        # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.policy_scheduler = torch.optim.lr_scheduler.StepLR(
            self.policy_optimizer, step_size=30, gamma=0.5
        )
        self.value_scheduler = torch.optim.lr_scheduler.StepLR(
            self.value_optimizer, step_size=30, gamma=0.5
        )

        self.gamma = 0.99
        self.lam = 0.95
        self.max_kl = 0.01
        
        # æ·»åŠ æ€§èƒ½è·Ÿè¸ª
        self.best_reward = -float('inf')
        self.best_model_state = None
        self.no_improvement_count = 0
        
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"å¹¶è¡Œç¯å¢ƒæ•°: {num_envs}")
        
    def select_actions_batch(self, states):
        """æ‰¹é‡é€‰æ‹©åŠ¨ä½œ - å…³é”®ä¼˜åŒ–ç‚¹"""
        states_tensor = torch.FloatTensor(states).to(self.device)
        with torch.no_grad():
            probs = self.policy(states_tensor)
            dist = Categorical(probs=probs)
            actions = dist.sample()
            log_probs = dist.log_prob(actions)
        return actions.cpu().numpy(), log_probs.cpu().numpy()
    
    def get_values_batch(self, states):
        """æ‰¹é‡è·å–ä»·å€¼ - å…³é”®ä¼˜åŒ–ç‚¹"""
        states_tensor = torch.FloatTensor(states).to(self.device)
        with torch.no_grad():
            values = self.value(states_tensor)
        return values.cpu().numpy()
    
    def compute_advantages(self, rewards, values, dones, last_values):
        """
        è®¡ç®—GAEä¼˜åŠ¿ - æ”¯æŒå¤šç¯å¢ƒçš„æ‰¹é‡è®¡ç®—
        
        Args:
            rewards: æ‰€æœ‰ç¯å¢ƒçš„å¥–åŠ±åˆ—è¡¨
            values: æ‰€æœ‰ç¯å¢ƒçš„ä»·å€¼åˆ—è¡¨
            dones: æ‰€æœ‰ç¯å¢ƒçš„doneæ ‡å¿—åˆ—è¡¨
            last_values: æ¯ä¸ªç¯å¢ƒçš„æœ€åä¸€ä¸ªä»·å€¼ (é•¿åº¦ä¸ºnum_envsçš„åˆ—è¡¨)
        
        Returns:
            advantages: torch.Tensor
        """
        # å°†æ•°æ®é‡ç»„ä¸ºæ¯ä¸ªç¯å¢ƒçš„è½¨è¿¹
        # rewards, values, dones çš„é•¿åº¦åº”è¯¥æ˜¯ num_envs çš„å€æ•°
        steps_per_env = len(rewards) // self.num_envs
        
        advantages = []
        all_env_advantages = []
        
        # ä¸ºæ¯ä¸ªç¯å¢ƒåˆ†åˆ«è®¡ç®—ä¼˜åŠ¿
        for env_idx in range(self.num_envs):
            # æå–è¯¥ç¯å¢ƒçš„æ•°æ® (äº¤é”™å­˜å‚¨)
            env_rewards = rewards[env_idx::self.num_envs]
            env_values = values[env_idx::self.num_envs]
            env_dones = dones[env_idx::self.num_envs]
            env_last_value = last_values[env_idx]
            
            # è®¡ç®—è¯¥ç¯å¢ƒçš„GAE
            env_advantages = []
            gae = 0
            
            for t in reversed(range(len(env_rewards))):
                if t == len(env_rewards) - 1:
                    next_value = env_last_value
                else:
                    next_value = env_values[t + 1]
                
                delta = env_rewards[t] + self.gamma * next_value * (1 - env_dones[t]) - env_values[t]
                gae = delta + self.gamma * self.lam * (1 - env_dones[t]) * gae
                env_advantages.insert(0, gae)
            
            all_env_advantages.append(env_advantages)
        
        for step_idx in range(steps_per_env):
            for env_idx in range(self.num_envs):
                advantages.append(all_env_advantages[env_idx][step_idx])
        
        return torch.FloatTensor(advantages).to(self.device)

    
    def collect_batch_vectorized(self, vec_env: VectorizedEnv, batch_size=1024):
        """å‘é‡åŒ–é‡‡æ · - æœ€å¤§ä¼˜åŒ–ç‚¹
        ä¿è¯æ¯ä¸ªç¯å¢ƒè‡³å°‘é‡‡æ ·ä¸€å±€å®Œæ•´æ¸¸æˆ
        """
        # é¢„åˆ†é…å†…å­˜
        total_steps = batch_size
        states_list = []
        actions_list = []
        rewards_list = []
        log_probs_list = []
        dones_list = []
        values_list = []
        
        episode_rewards = []
        episode_reward_trackers = [0.0] * self.num_envs
        
        # è·Ÿè¸ªæ¯ä¸ªç¯å¢ƒæ˜¯å¦å®Œæˆäº†è‡³å°‘ä¸€å±€æ¸¸æˆ
        env_completed_episodes = [0] * self.num_envs
        min_episodes_per_env = 1  # æ¯ä¸ªç¯å¢ƒè‡³å°‘å®Œæˆ1å±€
        
        # é‡ç½®ç¯å¢ƒ
        current_states = vec_env.reset()
        
        steps = 0
        # ä¿®æ”¹åœæ­¢æ¡ä»¶: éœ€è¦åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶
        # 1. è¾¾åˆ°æœ€å°æ­¥æ•° batch_size
        # 2. æ¯ä¸ªç¯å¢ƒéƒ½è‡³å°‘å®Œæˆäº†min_episodes_per_envå±€æ¸¸æˆ
        while True:
            # æ‰¹é‡é€‰æ‹©åŠ¨ä½œ
            actions, log_probs = self.select_actions_batch(current_states)
            
            # æ‰¹é‡è·å–ä»·å€¼
            values = self.get_values_batch(current_states)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_states, rewards, dones = vec_env.step(actions)
            
            # å­˜å‚¨æ•°æ®
            for i in range(self.num_envs):
                states_list.append(current_states[i])
                actions_list.append(actions[i])
                rewards_list.append(rewards[i])
                log_probs_list.append(log_probs[i])
                dones_list.append(dones[i])
                values_list.append(values[i])
                
                episode_reward_trackers[i] += rewards[i]
                
                if dones[i]:
                    episode_rewards.append(episode_reward_trackers[i])
                    episode_reward_trackers[i] = 0.0
                    env_completed_episodes[i] += 1
                
                steps += 1
            
            current_states = next_states
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if steps >= batch_size:
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç¯å¢ƒéƒ½å®Œæˆäº†è‡³å°‘min_episodes_per_envå±€
                if all(count >= min_episodes_per_env for count in env_completed_episodes):
                    break
        last_values = []
        for i in range(self.num_envs):
            if not dones[i]:
                last_value = self.get_values_batch(np.array([current_states[i]]))[0]
            else:
                last_value = 0.0
            last_values.append(last_value)
        return (states_list, 
                actions_list, 
                rewards_list, 
                log_probs_list, 
                dones_list, 
                values_list, 
                episode_rewards,
                last_values)
    
    def update(self, states, actions, advantages, old_log_probs, returns):
        """æ›´æ–°ç­–ç•¥å’Œä»·å€¼ç½‘ç»œ"""
        # è½¬æ¢ä¸ºå¼ é‡
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        actions_tensor = torch.LongTensor(actions).to(self.device)
        advantages = advantages.detach()
        old_log_probs_tensor = torch.FloatTensor(old_log_probs).to(self.device)
        
        # è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
        probs = self.policy(states_tensor)
        dist = Categorical(probs=probs)
        new_log_probs = dist.log_prob(actions_tensor)
        
        # è®¡ç®—KLæ•£åº¦
        kl = (old_log_probs_tensor - new_log_probs).mean()
        
        # å¦‚æœKLæ•£åº¦å¤ªå¤§,ä¸æ›´æ–°ç­–ç•¥
        if kl > self.max_kl * 1.5:
            policy_updated = False
            policy_loss_value = 0.0
        else:
            # è®¡ç®—ç­–ç•¥æŸå¤±
            ratio = torch.exp(new_log_probs - old_log_probs_tensor)
            policy_loss = -(ratio * advantages).mean()
            
            # ä½¿ç”¨ç®€å•çš„æ¢¯åº¦ä¸‹é™æ›´æ–°ç­–ç•¥
            self.policy_optimizer.zero_grad()
            policy_loss.backward()
            # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.3)
            self.policy_optimizer.step()
            policy_updated = True
            policy_loss_value = policy_loss.item()
        
        # æ›´æ–°ä»·å€¼ç½‘ç»œ
        value_loss_value = 0.0
        for i in range(1):
            values_pred = self.value(states_tensor)
            value_loss = F.mse_loss(values_pred, returns)
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.value.parameters(), 0.3)
            self.value_optimizer.step()
            value_loss_value = value_loss.item()
        
        return policy_updated, value_loss_value, kl.item(), policy_loss_value if policy_updated else 0.0
    
    def train(self, num_iterations=100, batch_size=1024, eval_step=100, save_path='trpo_fast_model.pth', enable_monitor=True):
        """è®­ç»ƒç®—æ³•"""
        vec_env = VectorizedEnv(num_envs=self.num_envs)
        
        # åˆ›å»ºç›‘æ§å™¨
        monitor = TrainingMonitor(window_size=10) if enable_monitor else None
        
        print("=" * 70)
        print("å¼€å§‹è®­ç»ƒä¼˜åŒ–ç‰ˆTRPO (å¹¶è¡Œé‡‡æ · + ç¨³å®šæ€§å¢å¼º)")
        print(f"è¿­ä»£æ¬¡æ•°: {num_iterations}, æ¯æ¬¡æ‰¹é‡å¤§å°: {batch_size}")
        print(f"åˆå§‹å­¦ä¹ ç‡: {self.policy_optimizer.param_groups[0]['lr']:.2e}")
        print(f"ç›‘æ§: {'âœ“ å¯ç”¨' if enable_monitor else 'âœ— ç¦ç”¨'}")
        print("=" * 70)
        
        pbar = tqdm.tqdm(range(num_iterations) if num_iterations > 0 else count(0), desc="Iteration")
        
        try:
            for iteration in pbar:
                start_time = time.time()
                
                # å‘é‡åŒ–é‡‡æ ·
                states, actions, rewards, log_probs, dones, values, episode_rewards, last_values = \
                    self.collect_batch_vectorized(vec_env, batch_size)
                
                sampling_time = time.time() - start_time
                actual_samples = len(states)
                
                # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
                advantages = self.compute_advantages(rewards, values, dones, last_values)
                returns = advantages + torch.FloatTensor(values).to(self.device)
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
                
                # æ›´æ–°ç½‘ç»œ
                update_start = time.time()
                policy_updated, value_loss, kl, policy_loss = self.update(
                    states, actions, advantages, log_probs, returns
                )
                update_time = time.time() - update_start
                
                # # æ›´æ–°å­¦ä¹ ç‡
                self.policy_scheduler.step()
                self.value_scheduler.step()
                
                # ç»Ÿè®¡ä¿¡æ¯
                avg_reward = np.mean(episode_rewards)
                current_lr = self.policy_optimizer.param_groups[0]['lr']
                
                # æ›´æ–°ç›‘æ§å™¨
                if monitor:
                    monitor.update(
                        iteration=iteration,
                        reward=avg_reward,
                        value_loss=value_loss,
                        policy_loss=policy_loss,
                        kl=kl,
                        lr=current_lr,
                        episode_count=len(episode_rewards)
                    )
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                if avg_reward > self.best_reward:
                    self.best_reward = avg_reward
                    # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                    self.best_model_state = {
                        'policy': self.policy.state_dict(),
                        'value': self.value.state_dict(),
                        'iteration': iteration,
                        'reward': avg_reward
                    }
                    self.save_model(save_path)
                    self.no_improvement_count = 0
                else:
                    self.no_improvement_count += 1
                
                pbar.set_postfix({
                    'Reward': f'{avg_reward:.1f}',
                    'Best': f'{self.best_reward:.1f}',
                    'Episodes': len(episode_rewards),
                    'Samples': actual_samples,
                    'Max': np.max(states),
                    'KL': kl
                })
                
                # å¦‚æœè¿ç»­å¤šæ¬¡æ²¡æœ‰æ”¹å–„ï¼Œé™ä½å­¦ä¹ ç‡æˆ–æ¢å¤æœ€ä½³æ¨¡å‹
                if self.no_improvement_count >= 100:
                    print(f"\nâš ï¸ æ€§èƒ½è¿ç»­ {self.no_improvement_count} æ¬¡è¿­ä»£æœªæ”¹å–„ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹...")
                    vec_env = VectorizedEnv(num_envs=self.num_envs)
                    if self.best_model_state is not None:
                        self.policy.load_state_dict(self.best_model_state['policy'])
                        self.value.load_state_dict(self.best_model_state['value'])
                        print(f"âœ“ å·²æ¢å¤åˆ°ç¬¬ {self.best_model_state['iteration']} æ¬¡è¿­ä»£çš„æ¨¡å‹ (å¥–åŠ±: {self.best_model_state['reward']:.1f})")
                    self.no_improvement_count = 0
                
                # æ¯10æ¬¡è¿­ä»£è¯„ä¼°ä¸€æ¬¡
                if (iteration + 1) % eval_step == 0:
                    self.evaluate(num_episodes=3)
        finally:
            vec_env.close()
            self.save_model(save_path)
        print("=" * 70)
        print("è®­ç»ƒå®Œæˆ!")
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if self.best_model_state is not None:
            print(f"\nâœ“ æ¢å¤æœ€ä½³æ¨¡å‹ (ç¬¬ {self.best_model_state['iteration']} æ¬¡è¿­ä»£, å¥–åŠ±: {self.best_model_state['reward']:.1f})")
            self.policy.load_state_dict(self.best_model_state['policy'])
            self.value.load_state_dict(self.best_model_state['value'])
        
        # ä¿å­˜ç›‘æ§æ•°æ®
        if monitor:
            print(monitor.get_summary())
            monitor.save_data("training_data.json")
            monitor.plot("training_monitor.png")
        
    def evaluate(self, num_episodes=5):
        """è¯„ä¼°ç­–ç•¥"""
        env = gym.make("Game2048-v0")
        
        print("\n" + "-" * 70)
        print("è¯„ä¼°ä¸­...")
        rewards = []
        max_tiles = []
        
        for ep in range(num_episodes):
            state, _ = env.reset()
            episode_reward = 0.0
            done = False
            
            while not done:
                action, _ = self.select_actions_batch(np.array([state]))
                state, reward, terminated, truncated, _ = env.step(action[0])
                done = terminated or truncated
                episode_reward += float(reward)
            
            max_tile = np.max(state)
            rewards.append(episode_reward)
            max_tiles.append(max_tile)
            print(f"  å›åˆ {ep+1}: å¥–åŠ±={episode_reward:8.1f}, æœ€å¤§æ–¹å—={int(max_tile)}")
        
        print(f"å¹³å‡å¥–åŠ±: {np.mean(rewards):8.1f}, å¹³å‡æœ€å¤§æ–¹å—: {np.mean(max_tiles):.1f}")
        print("-" * 70 + "\n")
        
        env.close()
    
    def save_model(self, path="fast_trpo_model.pth"):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy': self.policy.state_dict(),
            'value': self.value.state_dict(),
        }, path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load_model(self, path="fast_trpo_model.pth"):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.policy.load_state_dict(checkpoint['policy'])
        self.value.load_state_dict(checkpoint['value'])
        print(f"æ¨¡å‹å·²åŠ è½½: {path}")


def compare_sampling_speed():
    """å¯¹æ¯”é‡‡æ ·é€Ÿåº¦"""
    print("\n" + "=" * 70)
    print("é‡‡æ ·é€Ÿåº¦å¯¹æ¯”æµ‹è¯•")
    print("=" * 70)
    
    from trpo_game2048_simple import SimpleTRPO
    
    batch_size = 1024
    
    # æµ‹è¯•åŸå§‹ç‰ˆæœ¬
    print("\næµ‹è¯•1: åŸå§‹å•ç¯å¢ƒé‡‡æ ·")
    agent_single = SimpleTRPO()
    env = gym.make("Game2048-v0")
    
    start = time.time()
    agent_single.collect_batch(env, batch_size)
    single_time = time.time() - start
    env.close()
    
    print(f"æ—¶é—´: {single_time:.2f}ç§’")
    print(f"é‡‡æ ·é€Ÿåº¦: {batch_size / single_time:.1f} æ ·æœ¬/ç§’")
    
    # æµ‹è¯•å‘é‡åŒ–ç‰ˆæœ¬
    for num_envs in [2, 4, 8, 16, 32, 64]:
        print(f"\næµ‹è¯•{num_envs+1}: {num_envs}ä¸ªå¹¶è¡Œç¯å¢ƒ")
        agent_vec = FastTRPO(num_envs=num_envs)
        vec_env = VectorizedEnv(num_envs=num_envs)
        
        start = time.time()
        states, actions, rewards, log_probs, dones, values, episode_rewards, _ = \
            agent_vec.collect_batch_vectorized(vec_env, batch_size)
        vec_time = time.time() - start
        vec_env.close()
        
        print(f"æ—¶é—´: {vec_time:.2f}ç§’")
        print(f"é‡‡æ ·é€Ÿåº¦: {len(actions) / vec_time:.1f} æ ·æœ¬/ç§’")
        # print(f"åŠ é€Ÿæ¯”: {single_time / vec_time:.2f}x")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--compare':
        # å¯¹æ¯”é‡‡æ ·é€Ÿåº¦
        compare_sampling_speed()
    else:
        # è®­ç»ƒ
        print("\n" + "ğŸš€" * 35)
        print("ä¼˜åŒ–é‡‡æ ·é€Ÿåº¦çš„TRPOè®­ç»ƒ")
        print("ğŸš€" * 35)
        
        agent = FastTRPO(num_envs=64)
        
        # å¯é€‰: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        # agent.load_model("trpo_fast_model_314.7.pth")
        
        # è®­ç»ƒ
        agent.train(num_iterations=-1, batch_size=2048)
        
        # ä¿å­˜æ¨¡å‹
        agent.save_model("fast_trpo_model.pth")
        
        # æœ€ç»ˆè¯„ä¼°
        agent.evaluate(num_episodes=10)


if __name__ == "__main__":
    main()
