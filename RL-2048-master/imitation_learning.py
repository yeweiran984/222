"""
æ¨¡ä»¿å­¦ä¹ (Imitation Learning)
ä½¿ç”¨äººå·¥ç¤ºä¾‹è¿›è¡Œè¡Œä¸ºå…‹éš†(Behavioral Cloning)
ç„¶åç»§ç»­ç”¨TRPOè¿›è¡Œå¼ºåŒ–å­¦ä¹ 
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import gymnasium as gym
import game2048
import pickle
import argparse
from trpo_game2048_simple import SimpleTRPO, SimplePolicy, SimpleValue
from fast_trpo import FastTRPO


class ExpertDataset(Dataset):
    """ä¸“å®¶ç¤ºä¾‹æ•°æ®é›†"""
    
    def __init__(self, trajectories):
        self.states = []
        self.actions = []
        
        # ä»è½¨è¿¹ä¸­æå–æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹
        for traj in trajectories:
            for state, action in zip(traj['states'], traj['actions']):
                self.states.append(state)
                self.actions.append(action)
        
        self.states = np.array(self.states)
        self.actions = np.array(self.actions)
        
        print(f"æ•°æ®é›†å¤§å°: {len(self.states)} ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹")
    
    def __len__(self):
        return len(self.states)
    
    def __getitem__(self, idx):
        state = torch.FloatTensor(self.states[idx])
        action = torch.LongTensor([self.actions[idx]])
        return state, action


class ImitationLearning:
    """æ¨¡ä»¿å­¦ä¹  + TRPOå¼ºåŒ–å­¦ä¹ """
    
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu', impl='fast'):
        self.device = device
        self.impl = impl
        if impl == 'fast':
            self.agent = FastTRPO(num_envs=64, device=device)
        elif impl == 'basic':
            self.agent = SimpleTRPO(device=device)
        else:
            raise ValueError("æœªçŸ¥çš„TRPOå®ç°æ–¹å¼,è¯·é€‰æ‹© 'fast' æˆ– 'basic'")
        
    def load_expert_data(self, filename):
        """åŠ è½½ä¸“å®¶ç¤ºä¾‹æ•°æ®"""
        print(f"åŠ è½½ä¸“å®¶æ•°æ®: {filename}")
        with open(filename, 'rb') as f:
            data = pickle.load(f)
        
        trajectories = data['trajectories']
        print(f"åŠ è½½äº† {len(trajectories)} å±€æ¸¸æˆçš„è½¨è¿¹")
        
        # ç»Ÿè®¡ä¿¡æ¯
        rewards = [t['total_reward'] for t in trajectories]
        steps = [t['steps'] for t in trajectories]
        max_tiles = [t['max_tile'] for t in trajectories]
        
        print(f"ä¸“å®¶æ•°æ®ç»Ÿè®¡:")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.1f}")
        print(f"  å¹³å‡æ­¥æ•°: {np.mean(steps):.1f}")
        print(f"  æœ€å¤§æ–¹å—: {np.max(max_tiles)}")
        
        return trajectories
    
    def behavioral_cloning(self, trajectories, epochs=50, batch_size=4096, lr=1e-3):
        """è¡Œä¸ºå…‹éš†è®­ç»ƒ"""
        print("\n" + "=" * 70)
        print("é˜¶æ®µ 1: è¡Œä¸ºå…‹éš† (Behavioral Cloning)")
        print("=" * 70)
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = ExpertDataset(trajectories)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # åˆ›å»ºä¼˜åŒ–å™¨(åªè®­ç»ƒç­–ç•¥ç½‘ç»œ)
        optimizer = torch.optim.Adam(self.agent.policy.parameters(), lr=lr)
        
        # è®­ç»ƒ
        best_loss = float('inf')
        best_epoch = 1
        avg_accuracy = 0.0
        
        for epoch in range(epochs):
            total_loss = 0
            total_accuracy = 0
            num_batches = 0
            
            for states, actions in dataloader:
                states = states.to(self.device)
                actions = actions.squeeze().to(self.device)
                
                # å‰å‘ä¼ æ’­
                probs = self.agent.policy(states)
                
                # è®¡ç®—äº¤å‰ç†µæŸå¤±
                loss = F.cross_entropy(probs, actions)
                
                # è®¡ç®—å‡†ç¡®ç‡
                pred_actions = torch.argmax(probs, dim=1)
                accuracy = (pred_actions == actions).float().mean()
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.agent.policy.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
                total_accuracy += accuracy.item()
                num_batches += 1
            
            avg_loss = total_loss / num_batches
            avg_accuracy = total_accuracy / num_batches
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                best_epoch = epoch + 1
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}/{epochs} | "
                      f"æŸå¤±: {avg_loss:.4f} | "
                      f"å‡†ç¡®ç‡: {avg_accuracy*100:.2f}% | "
                      f"æœ€ä½³: {best_loss:.4f} (Epoch {best_epoch})")
        
        print(f"\nè¡Œä¸ºå…‹éš†å®Œæˆ!")
        print(f"æœ€ç»ˆå‡†ç¡®ç‡: {avg_accuracy*100:.2f}%")
        
        # # è¯„ä¼°æ¨¡ä»¿å­¦ä¹ åçš„ç­–ç•¥
        # print("\nè¯„ä¼°è¡Œä¸ºå…‹éš†åçš„ç­–ç•¥:")
        # env = gym.make("Game2048-v0")
        # self.agent.evaluate(env, num_episodes=5)
        # env.close()
    
    def continue_with_rl(self, save_path, num_iterations=50, batch_size=1024):
        """ç»§ç»­ä½¿ç”¨TRPOå¼ºåŒ–å­¦ä¹ """
        print("\n" + "=" * 70)
        print("é˜¶æ®µ 2: TRPOå¼ºåŒ–å­¦ä¹ ")
        print("=" * 70)
        print("ä»æ¨¡ä»¿å­¦ä¹ çš„ç­–ç•¥å¼€å§‹,ç»§ç»­ç”¨TRPOä¼˜åŒ–...\n")
        
        # ä½¿ç”¨TRPOç»§ç»­è®­ç»ƒ
        self.agent.train(num_iterations=num_iterations, batch_size=batch_size, save_path=save_path)

    def full_pipeline(self, expert_data_file, save_path,
                      bc_epochs=50, bc_batch_size=64, bc_lr=1e-3,
                      rl_iterations=50, rl_batch_size=1024):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹: æ¨¡ä»¿å­¦ä¹  -> å¼ºåŒ–å­¦ä¹ """
        print("\n" + "ğŸš€" * 35)
        print("æ¨¡ä»¿å­¦ä¹  + å¼ºåŒ–å­¦ä¹  å®Œæ•´è®­ç»ƒæµç¨‹")
        print("ğŸš€" * 35)
        
        # é˜¶æ®µ1: åŠ è½½æ•°æ®
        print("\n" + "=" * 70)
        print("å‡†å¤‡é˜¶æ®µ: åŠ è½½ä¸“å®¶æ•°æ®")
        print("=" * 70)
        trajectories = self.load_expert_data(expert_data_file)
        
        # é˜¶æ®µ2: è¡Œä¸ºå…‹éš†
        self.behavioral_cloning(
            trajectories, 
            epochs=bc_epochs, 
            batch_size=bc_batch_size, 
            lr=bc_lr
        )
        
        # ä¿å­˜è¡Œä¸ºå…‹éš†åçš„æ¨¡å‹
        print("\nä¿å­˜è¡Œä¸ºå…‹éš†åçš„æ¨¡å‹...")
        self.agent.save_model("model_after_bc.pth")
        
        # é˜¶æ®µ3: å¼ºåŒ–å­¦ä¹ 
        self.continue_with_rl(
            save_path=save_path,
            num_iterations=rl_iterations, 
            batch_size=rl_batch_size
        )
        
        # # æœ€ç»ˆè¯„ä¼°
        # print("\n" + "=" * 70)
        # print("æœ€ç»ˆè¯„ä¼°")
        # print("=" * 70)
        # env: game2048.Game2048Env = gym.make("Game2048-v0") # type: ignore
        # self.agent.evaluate(env, num_episodes=10)
        # env.close()
        
        print("\n" + "=" * 70)
        print("âœ… è®­ç»ƒæµç¨‹å®Œæˆ!")
        print("=" * 70)
        print("\nç”Ÿæˆçš„æ¨¡å‹:")
        print("  1. model_after_bc.pth - è¡Œä¸ºå…‹éš†åçš„æ¨¡å‹")
        print("  2. model_after_bc_and_rl.pth - å¼ºåŒ–å­¦ä¹ åçš„æœ€ç»ˆæ¨¡å‹")
        print("\nä½¿ç”¨ play_trpo.py å¯ä»¥è§‚çœ‹è®­ç»ƒæ•ˆæœ")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ¨¡ä»¿å­¦ä¹  + TRPOå¼ºåŒ–å­¦ä¹ ')
    parser.add_argument('--data', type=str, help='ä¸“å®¶æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--save-path', type=str, default='model_after_bc_and_rl.pth', help='æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--impl', type=str, default='fast', choices=['fast', 'basic'], help='TRPOå®ç°æ–¹å¼')
    parser.add_argument('--bc-epochs', type=int, default=25, help='è¡Œä¸ºå…‹éš†è®­ç»ƒè½®æ•°')
    parser.add_argument('--bc-batch-size', type=int, default=4096, help='è¡Œä¸ºå…‹éš†æ‰¹é‡å¤§å°')
    parser.add_argument('--bc-lr', type=float, default=1e-4, help='è¡Œä¸ºå…‹éš†å­¦ä¹ ç‡')
    parser.add_argument('--rl-iterations', type=int, default=100, help='å¼ºåŒ–å­¦ä¹ è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--rl-batch-size', type=int, default=2048, help='å¼ºåŒ–å­¦ä¹ æ‰¹é‡å¤§å°')
    parser.add_argument('--compare', action='store_true', help='å¯¹æ¯”ä¸åŒæ¨¡å‹')
    parser.add_argument('--bc-only', action='store_true', help='ä»…è¿›è¡Œè¡Œä¸ºå…‹éš†')
    
    args = parser.parse_args()
    
    if args.data:
        # è®­ç»ƒ
        il = ImitationLearning()
        
        if args.bc_only:
            # ä»…è¡Œä¸ºå…‹éš†
            trajectories = il.load_expert_data(args.data)
            il.behavioral_cloning(
                trajectories,
                epochs=args.bc_epochs,
                batch_size=args.bc_batch_size,
                lr=args.bc_lr
            )
            il.agent.save_model("model_after_bc.pth")
        else:
            # å®Œæ•´æµç¨‹
            il.full_pipeline(
                expert_data_file=args.data,
                save_path=args.save_path,
                bc_epochs=args.bc_epochs,
                bc_batch_size=args.bc_batch_size,
                bc_lr=args.bc_lr,
                rl_iterations=args.rl_iterations,
                rl_batch_size=args.rl_batch_size
            )
    else:
        print(parser.usage)


if __name__ == "__main__":
    main()
