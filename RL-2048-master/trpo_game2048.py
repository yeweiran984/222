import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import gymnasium as gym
import game2048
import os
import re
from collections import deque
import time
import tqdm
import argparse
from itertools import count
from trpo_game2048_simple import SimplePolicy, SimpleValue
from vectorized_env_mp import VectorizedEnv as MPVectorizedEnv


class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, input_dim, hidden_dim, action_dim):
        super(PolicyNetwork, self).__init__()
        # 2048æ¸¸æˆçš„è§‚å¯Ÿç©ºé—´æ˜¯4x4çš„ç½‘æ ¼,éœ€è¦å±•å¹³
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, x):
        if len(x.shape) > 2:
            x = x.reshape(x.shape[0], -1)
        else:
            x = x.reshape(-1)
        x = torch.log2(x.float() + 1)
        
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        logits = self.fc3(x)
        return logits
    
    def get_action(self, state):
        """è·å–åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡"""
        logits = self.forward(state)
        dist = Categorical(logits=logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob
    
    def evaluate(self, states, actions):
        """è¯„ä¼°çŠ¶æ€-åŠ¨ä½œå¯¹"""
        logits = self.forward(states)
        dist = Categorical(logits=logits)
        log_probs = dist.log_prob(actions)
        entropy = dist.entropy()
        return log_probs, entropy


class ValueNetwork(nn.Module):
    """ä»·å€¼ç½‘ç»œ"""
    def __init__(self, input_dim, hidden_dim):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        
    def forward(self, x):
        if len(x.shape) > 2:
            x = x.reshape(x.shape[0], -1)
        else:
            x = x.reshape(-1)
        x = torch.log2(x.float() + 1)
        
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        value = self.fc3(x)
        return value.squeeze(-1)

class VectorizedEnv:
    """å‘é‡åŒ–ç¯å¢ƒåŒ…è£…å™¨ - å¹¶è¡Œè¿è¡Œå¤šä¸ªç¯å¢ƒ"""
    def __init__(self, num_envs: int = 4, target=2048):
        self.num_envs = num_envs
        self.envs = [gym.make("Game2048-v0", target=target) for _ in range(num_envs)]
    
    def reset(self):
        states = []
        for env in self.envs:
            state, _ = env.reset()
            states.append(state)
        return np.array(states)
    
    def step(self, actions):
        states, rewards, dones = [], [], []
        for env, action in zip(self.envs, actions):
            state, reward, terminated, truncated, _ = env.step(int(action))
            done = terminated or truncated
            if done:
                state, _ = env.reset()
            states.append(state)
            rewards.append(reward)
            dones.append(done)
        return np.array(states), np.array(rewards), np.array(dones)
    
    def close(self):
        for env in self.envs:
            env.close()

    def reset_one(self, idx: int):
        """ä»…é‡ç½®ä¸€ä¸ªå­ç¯å¢ƒå¹¶è¿”å›å…¶åˆå§‹è§‚æµ‹ã€‚"""
        i = int(idx)
        if i < 0 or i >= self.num_envs:
            raise IndexError("env index out of range")
        obs, _ = self.envs[i].reset()
        return obs

class TRPO:
    """TRPOç®—æ³•å®ç°"""
    def __init__(
        self,
        env,
        hidden_dim=256,
        gamma=0.99,
        lam=0.95,
        max_kl=0.01,
        damping=1e-2,
        entropy_coeff = 0.01,
        value_lr=1e-3,
        train_value_iters=10,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        num_envs=1,
        vec_impl: str = 'auto',  # 'auto' | 'inproc' | 'mp'
        env_target: int = 2048,
    ):
        self.env = env
        self.gamma = gamma
        self.lam = lam
        self.max_kl = max_kl
        self.damping = damping
        self.entropy_coeff = entropy_coeff
        self.value_lr = value_lr
        self.train_value_iters = train_value_iters
        self.device = device
        self.num_envs = int(max(1, num_envs))
        self.vec_impl = vec_impl
        self.env_target = int(env_target)
        self.start_iter = 0  # å°†åœ¨åŠ è½½æ¨¡å‹æ—¶å°è¯•ä»æ–‡ä»¶åè§£æ
        
        # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
        self.state_dim = np.prod(env.observation_space.shape)
        self.action_dim = env.action_space.n
        
        # åˆå§‹åŒ–ç½‘ç»œ
        # self.policy = PolicyNetwork(self.state_dim, hidden_dim, self.action_dim).to(device)
        # self.value_net = ValueNetwork(self.state_dim, hidden_dim).to(device)

        self.policy = SimplePolicy().to(device)
        self.value_net = SimpleValue().to(device)
        
        # ä»·å€¼ç½‘ç»œä¼˜åŒ–å™¨
        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=value_lr)
        
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"çŠ¶æ€ç»´åº¦: {self.state_dim}, åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"å¹¶è¡Œç¯å¢ƒæ•°: {self.num_envs}")
    
    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_t = torch.as_tensor(state, dtype=torch.float32, device=self.device)
        if state_t.dim() == 2:  # 4x4 -> [1,4,4]
            state_t = state_t.unsqueeze(0)
        with torch.no_grad():
            probs = self.policy(state_t)            # [1, A]
            dist = Categorical(probs=probs.squeeze(0))
            action = dist.sample()
            log_prob = dist.log_prob(action)
        return int(action.item()), float(log_prob.item())

    def select_actions_batch(self, states):
        """æ‰¹é‡é€‰æ‹©åŠ¨ä½œ"""
        states_tensor = torch.as_tensor(np.array(states), dtype=torch.float32, device=self.device)
        with torch.no_grad():
            probs = self.policy(states_tensor)      # [B, A]
            dist = Categorical(probs=probs)
            actions = dist.sample()
            log_probs = dist.log_prob(actions)
        return actions.cpu().numpy(), log_probs.cpu().numpy()

    def get_values_batch(self, states):
        """æ‰¹é‡è·å–ä»·å€¼"""
        states_tensor = torch.as_tensor(np.array(states), dtype=torch.float32, device=self.device)
        with torch.no_grad():
            values = self.value_net(states_tensor)  # å½¢å¦‚ [B] æˆ– [B,1]ï¼ŒSimpleValue ä¸­å·²å¤„ç†
        return values.detach().cpu().numpy()
    
    def compute_advantages(self, rewards, values, dones, last_value):
        """ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•° - å•ç¯å¢ƒ"""
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = last_value
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.lam * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = advantages + torch.FloatTensor(values).to(self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return advantages, returns

    def compute_advantages_vectorized(self, rewards, values, dones, last_values):
        """ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•° - å¤šç¯å¢ƒæ‰¹é‡"""
        steps_per_env = len(rewards) // self.num_envs
        all_env_adv = []
        for env_idx in range(self.num_envs):
            env_rewards = rewards[env_idx::self.num_envs]
            env_values = values[env_idx::self.num_envs]
            env_dones = dones[env_idx::self.num_envs]
            env_last_value = last_values[env_idx]
            env_adv = []
            gae = 0.0
            for t in reversed(range(len(env_rewards))):
                next_value = env_last_value if t == len(env_rewards) - 1 else env_values[t + 1]
                delta = env_rewards[t] + self.gamma * next_value * (1 - env_dones[t]) - env_values[t]
                gae = delta + self.gamma * self.lam * (1 - env_dones[t]) * gae
                env_adv.insert(0, gae)
            all_env_adv.append(env_adv)
        # äº¤é”™å›åˆ°åŸé¡ºåº
        advantages = []
        for step_idx in range(steps_per_env):
            for env_idx in range(self.num_envs):
                advantages.append(all_env_adv[env_idx][step_idx])
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = advantages + torch.FloatTensor(values).to(self.device)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        return advantages, returns
    
    def collect_trajectories(self, num_steps):
        """æ”¶é›†è½¨è¿¹æ•°æ® - å•ç¯å¢ƒ"""
        states = []
        actions = []
        rewards = []
        log_probs = []
        dones = []
        values = []
        
        state, _ = self.env.reset()
        episode_reward = 0
        episode_rewards = []
        
        for step in range(num_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob = self.select_action(state)
            
            # è·å–ä»·å€¼ä¼°è®¡
            state_tensor = torch.as_tensor(state, dtype=torch.float32, device=self.device)
            if state_tensor.dim() == 2:
                state_tensor = state_tensor.unsqueeze(0)  # [1,4,4]
            with torch.no_grad():
                value = self.value_net(state_tensor).squeeze(0).cpu().item()
            
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = self.env.step(action)
            done = terminated or truncated
            
            # å­˜å‚¨æ•°æ®
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            log_probs.append(log_prob)
            dones.append(done)
            values.append(value)
            
            episode_reward += reward
            state = next_state
            
            if done:
                episode_rewards.append(episode_reward)
                state, _ = self.env.reset()
                episode_reward = 0
        
        # è®¡ç®—æœ€åä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼ˆå¦‚æœæœ€åä¸€æ­¥æ˜¯doneï¼Œåˆ™ä¸º0ï¼‰
        if len(dones) > 0 and dones[-1]:
            last_value = 0.0
        else:
            st = torch.as_tensor(state, dtype=torch.float32, device=self.device)
            if st.dim() == 2:
                st = st.unsqueeze(0)  # [1,4,4]
            last_value = self.value_net(st).squeeze(0).cpu().item()
        
        return states, actions, rewards, log_probs, dones, values, episode_rewards, last_value

    def collect_batch_vectorized(self, vec_env, batch_size=1024, min_episodes_per_env=1):
        """å‘é‡åŒ–æ‰¹é‡é‡‡æ ·"""
        states_list, actions_list, rewards_list = [], [], []
        log_probs_list, dones_list, values_list = [], [], []
        episode_rewards, episode_trackers = [], [0.0] * vec_env.num_envs
        env_completed = [0] * vec_env.num_envs
        env_steps = [0] * vec_env.num_envs  # æ¯ä¸ªç¯å¢ƒè‡ªä¸Šæ¬¡å®Œæˆä»¥æ¥çš„æ­¥æ•°

        current_states = vec_env.reset()
        steps = 0
        while True:
            actions, log_probs = self.select_actions_batch(current_states)
            values = self.get_values_batch(current_states)
            next_states, rewards, dones = vec_env.step(actions)

            for i in range(vec_env.num_envs):
                states_list.append(current_states[i])
                actions_list.append(int(actions[i]))
                rewards_list.append(float(rewards[i]))
                log_probs_list.append(float(log_probs[i]))
                dones_list.append(bool(dones[i]))
                values_list.append(float(values[i]))
                episode_trackers[i] += float(rewards[i])
                env_steps[i] += 1
                if dones[i]:
                    episode_rewards.append(episode_trackers[i])
                    episode_trackers[i] = 0.0
                    env_completed[i] += 1
                    env_steps[i] = 0
                elif env_steps[i] >= int(batch_size):
                    # è‹¥å•ä¸ªç¯å¢ƒå·²äº§ç”Ÿ batch_size æ¬¡äº¤äº’ä½†ä»æœªç»“æŸï¼Œåˆ™å¼ºåˆ¶é‡ç½®å¹¶è®¡ä¸ºå®Œæˆä¸€æ¬¡
                    # å°è¯•ä»…é‡ç½®è¯¥ç¯å¢ƒ
                    if hasattr(vec_env, 'reset_one'):
                        try:
                            forced_obs = vec_env.reset_one(i)
                            next_states[i] = forced_obs
                        except Exception:
                            # å¤±è´¥æ—¶é€€åŒ–ä¸ºæ•´ä½“ resetï¼ˆä»£ä»·è¾ƒå¤§ï¼Œä½†ä¿è¯æ­£ç¡®ï¼‰
                            next_states = vec_env.reset()
                    else:
                        # æ— å•ç¯å¢ƒé‡ç½®èƒ½åŠ›ï¼Œé€€åŒ–ä¸ºæ•´ä½“ reset
                        next_states = vec_env.reset()

                    episode_rewards.append(episode_trackers[i])
                    episode_trackers[i] = 0.0
                    env_completed[i] += 1
                    env_steps[i] = 0
                steps += 1

            current_states = next_states
            if steps >= batch_size * 10:
                break
            if steps >= batch_size and all(c >= min_episodes_per_env for c in env_completed):
                break

        # è®¡ç®—æ¯ä¸ªenvçš„æœ€åä»·å€¼
        last_values = []
        for i in range(vec_env.num_envs):
            if not dones[i]:
                v = self.get_values_batch(current_states[i][None, ...])
                last_values.append(float(np.asarray(v).reshape(-1)[0]))
            else:
                last_values.append(0.0)

        return (states_list, actions_list, rewards_list, log_probs_list,
                dones_list, values_list, episode_rewards, last_values)

    def compute_policy_loss(self, states, actions, advantages, old_log_probs):
        """è®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆåŸºäº probs çš„ Categoricalï¼‰"""
        probs = self.policy(states)                  # [B, A]
        dist = Categorical(probs=probs)
        new_log_probs = dist.log_prob(actions)       # [B]
        entropy = dist.entropy().mean()
        ratio = torch.exp(new_log_probs - old_log_probs)
        policy_loss = -(ratio * advantages).mean() - self.entropy_coeff * entropy
        return policy_loss, new_log_probs
    
    # def compute_kl_divergence(self, states, actions, old_log_probs):
    #     """è®¡ç®—KLæ•£åº¦"""
    #     new_log_probs, _ = self.policy.evaluate(states, actions)
    #     # KL(old||new) = E[log(old) - log(new)]
    #     kl = (old_log_probs - new_log_probs).mean()
    #     return kl
    
    def flat_grad(self, grads, params):
        flat = []
        for g, p in zip(grads, params):
            if g is None:
                flat.append(torch.zeros_like(p).view(-1))
            else:
                flat.append(g.contiguous().view(-1))
        return torch.cat(flat)
    
    def flat_params(self, model):
        """è·å–å±•å¹³çš„å‚æ•°"""
        return torch.cat([param.data.reshape(-1) for param in model.parameters()])
    
    def set_flat_params(self, model, flat_params):
        """è®¾ç½®å±•å¹³çš„å‚æ•°"""
        idx = 0
        for param in model.parameters():
            param_length = param.numel()
            param.data.copy_(flat_params[idx:idx + param_length].view(param.shape))
            idx += param_length
    
    def conjugate_gradient(self, Avp_func, b, num_steps=10, tol=1e-10):
        """å…±è½­æ¢¯åº¦æ³•"""
        x = torch.zeros_like(b)
        r = b.clone()
        p = b.clone()
        rdotr = torch.dot(r, r)
        
        for _ in range(num_steps):
            Avp = Avp_func(p)
            alpha = rdotr / (torch.dot(p, Avp) + 1e-8)
            x += alpha * p
            r -= alpha * Avp
            new_rdotr = torch.dot(r, r)
            
            if new_rdotr < tol:
                break
            
            beta = new_rdotr / rdotr
            p = r + beta * p
            rdotr = new_rdotr
        
        return x
    
    def compute_fisher_vector_product(self, states, actions, vector):
        """
        è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µä¸å‘é‡çš„ä¹˜ç§¯
        ä½¿ç”¨ KL(old || new) çš„ Hessianï¼ˆåŸºäº probsï¼‰
        """
        probs = self.policy(states)                  # å½“å‰åˆ†å¸ƒ
        with torch.no_grad():
            old_probs = probs.detach()               # å›ºå®šæ—§åˆ†å¸ƒ

        # KL(old||new) çš„ä¸å‚æ•°ç›¸å…³éƒ¨åˆ†ï¼š-sum(old * log(new))
        kl = -(old_probs * torch.log(probs + 1e-10)).sum(-1).mean()

        kl_grad = torch.autograd.grad(kl, list(self.policy.parameters()),
                                      create_graph=True, retain_graph=True)
        flat_kl_grad = self.flat_grad(kl_grad, self.policy.parameters())

        grad_vector_product = torch.sum(flat_kl_grad * vector)
        hvp = torch.autograd.grad(grad_vector_product, list(self.policy.parameters()),
                                  retain_graph=True)
        flat_hvp = self.flat_grad(hvp, self.policy.parameters())
        return flat_hvp + self.damping * vector
    
    def line_search(self, states, actions, advantages, old_log_probs, 
                    full_step, expected_improve, max_backtracks=10):
        """çº¿æœç´¢æ‰¾åˆ°æ»¡è¶³KLçº¦æŸçš„æ­¥é•¿"""
        old_params = self.flat_params(self.policy)
        old_loss, _ = self.compute_policy_loss(states, actions, advantages, old_log_probs)
        
        for step_frac in [0.5**x for x in range(max_backtracks)]:
            new_params = old_params + step_frac * full_step
            self.set_flat_params(self.policy, new_params)

            with torch.no_grad():
                probs = self.policy(states)
                dist = Categorical(probs=probs)
                new_log_probs = dist.log_prob(actions)
                kl = (old_log_probs - new_log_probs).mean()

            new_loss, _ = self.compute_policy_loss(states, actions, advantages, old_log_probs)
            actual_improve = old_loss - new_loss
            expected_improve_frac = expected_improve * step_frac
            improvement_ratio = actual_improve / (expected_improve_frac + 1e-8)
            
            if kl <= self.max_kl and actual_improve > 0 and improvement_ratio > 0.1:
                return True
        
        self.set_flat_params(self.policy, old_params)
        return False
    
    def update_policy(self, states, actions, advantages, old_log_probs):
        """ä½¿ç”¨TRPOæ›´æ–°ç­–ç•¥"""
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        advantages = advantages.detach()
        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)
        
        # è®¡ç®—ç­–ç•¥æŸå¤±å’Œæ¢¯åº¦
        loss, _ = self.compute_policy_loss(states, actions, advantages, old_log_probs)
        grads = torch.autograd.grad(loss, self.policy.parameters(), retain_graph=True) # type: ignore
        loss_grad = self.flat_grad(grads, self.policy.parameters())
        
        # ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•æ±‚è§£æœç´¢æ–¹å‘
        def Avp_func(v):
            return self.compute_fisher_vector_product(states, actions, v)
        
        step_dir = self.conjugate_gradient(Avp_func, -loss_grad, num_steps=10)
        
        # è®¡ç®—æ­¥é•¿
        shs = 0.5 * torch.dot(step_dir, Avp_func(step_dir))
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if shs < 0:
            print(f"  è­¦å‘Š: shs={shs:.6f} < 0, ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ–¹å‘")
            # ä½¿ç”¨ç®€å•çš„æ¢¯åº¦ä¸‹é™æ–¹å‘
            step_dir = -loss_grad
            shs = 0.5 * torch.dot(step_dir, Avp_func(step_dir))
            
            # å¦‚æœè¿˜æ˜¯è´Ÿæ•°ï¼Œè·³è¿‡æ›´æ–°
            if shs < 0:
                print(f"  è­¦å‘Š: æ¢¯åº¦ä¸‹é™æ–¹å‘ä¹Ÿå¤±è´¥, è·³è¿‡ç­–ç•¥æ›´æ–°")
                return False
        
        lm = torch.sqrt(shs / self.max_kl)
        full_step = step_dir / (lm + 1e-8)
        
        # çº¿æœç´¢
        expected_improve = -torch.dot(loss_grad, full_step)
        success = self.line_search(states, actions, advantages, old_log_probs,
                                   full_step, expected_improve)
        
        return success
    
    def update_value(self, states, returns):
        """æ›´æ–°ä»·å€¼ç½‘ç»œ"""
        states = torch.FloatTensor(np.array(states)).to(self.device)
        returns = returns.detach()
        
        for _ in range(self.train_value_iters):
            values = self.value_net(states)
            value_loss = F.mse_loss(values, returns)
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
        
        return value_loss.item() # type: ignore

    def train(self, total_steps=100000, steps_per_update=2048, save_path=None):
        """è®­ç»ƒTRPOç®—æ³•
        
        Args:
            total_steps: æ€»è®­ç»ƒæ­¥æ•°ï¼Œ-1è¡¨ç¤ºæ— é™è®­ç»ƒç›´åˆ°Ctrl+Cä¸­æ–­
            steps_per_update: æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
        """
        print("=" * 70)
        print("å¼€å§‹è®­ç»ƒTRPOç®—æ³•")
        print(f"æ€»æ­¥æ•°: {'âˆ (æ— é™è®­ç»ƒ)' if total_steps == -1 else total_steps}, æ¯æ¬¡æ›´æ–°æ­¥æ•°: {steps_per_update}")
        if total_steps != -1:
            print(f"é¢„è®¡æ›´æ–°æ¬¡æ•°: {total_steps // steps_per_update}")
        else:
            print("æŒ‰ Ctrl+C åœæ­¢è®­ç»ƒ")
        print("=" * 70)
        
        episode_rewards = deque(maxlen=100)
        step_count = 0
        update_count = 0
        
        start_time = time.time()
        
        # åˆ›å»ºè¿›åº¦æ¡
        if total_steps == -1:
            # æ— é™è®­ç»ƒæ¨¡å¼ï¼Œä» start_iter å¼€å§‹è®¡æ•°
            pbar = tqdm.tqdm(count(self.start_iter), desc="è®­ç»ƒè¿›åº¦")
            total_updates = None
        else:
            # å›ºå®šæ­¥æ•°æ¨¡å¼ï¼šä»¥â€œæ›´æ–°æ¬¡æ•°â€ä¸ºå•ä½çš„è¿­ä»£å™¨ï¼Œä» start_iter åˆ° total_updates-1
            total_updates = max(1, total_steps // steps_per_update)
            pbar = tqdm.tqdm(range(self.start_iter, total_updates), desc="è®­ç»ƒè¿›åº¦")

        vec_env = None
        if self.num_envs > 1:
            impl = self.vec_impl
            if impl == 'auto':
                impl = 'mp'  # é»˜è®¤åœ¨å¤šç¯å¢ƒæ—¶ä½¿ç”¨å¤šè¿›ç¨‹ç‰ˆ
            if impl == 'mp':
                vec_env = MPVectorizedEnv(num_envs=self.num_envs, target=self.env_target)
            else:
                vec_env = VectorizedEnv(num_envs=self.num_envs, target=self.env_target)
        
        try:
            for _ in pbar:
                if total_steps != -1 and step_count >= total_steps:
                    break
                
                t0 = time.time()
                if vec_env is None:
                    # å•ç¯å¢ƒé‡‡æ ·
                    states, actions, rewards, log_probs, dones, values, ep_rewards, last_value = \
                        self.collect_trajectories(steps_per_update)
                    t_collect = time.time()
                    # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
                    advantages, returns = self.compute_advantages(rewards, values, dones, last_value)
                else:
                    # å‘é‡åŒ–æ‰¹é‡é‡‡æ ·
                    states, actions, rewards, log_probs, dones, values, ep_rewards, last_values = \
                        self.collect_batch_vectorized(vec_env, steps_per_update, 1)
                    t_collect = time.time()
                    advantages, returns = self.compute_advantages_vectorized(
                        rewards, values, dones, last_values
                    )
                    
                t_sample = time.time()

                step_count += len(states)
                episode_rewards.extend(ep_rewards)
                
                t_stat = time.time()
                    
                # æ›´æ–°ç­–ç•¥
                success = self.update_policy(states, actions, advantages, log_probs)
                
                # æ›´æ–°ä»·å€¼ç½‘ç»œ
                value_loss = self.update_value(states, returns)
                
                update_count += 1
                
                t_update = time.time()
                
                # æ›´æ–°è¿›åº¦æ¡
                # with torch.no_grad():
                #     st_tensor = torch.FloatTensor(np.array(states)).to(self.device)
                #     probs_new = self.policy(st_tensor)
                #     dist_new = Categorical(probs=probs_new)
                #     new_log_probs = dist_new.log_prob(torch.LongTensor(actions).to(self.device))
                #     kl = (torch.FloatTensor(log_probs).to(self.device) - new_log_probs).mean().item()
                
                avg_reward = np.mean(rewards)
                max_tile = int(np.max(states))
                
                states_arr = np.array(states)
                dones_arr = np.array(dones, dtype=bool)
                end_states = states_arr[dones_arr]
                avg_max_tile = np.mean(np.max(end_states, axis=(1,2)))
                
                pbar.set_postfix({
                    'å¹³å‡å¥–åŠ±': f'{avg_reward:.1f}',
                    'æœ€å¤§å—': max_tile,
                    'å¹³å‡æœ€å¤§å—': f'{avg_max_tile:.1f}',
                    'æ— æ•ˆåŠ¨ä½œ': f'{np.sum(np.array(rewards) == -10.0) / len(rewards) * 100:.1f}%',
                    'ä»·å€¼æŸå¤±': f'{value_loss:.1f}',
                    'ç­–ç•¥': 'âœ“' if success else 'âœ—'
                })

                # pbar.write(f"é‡‡æ ·æ—¶é—´ï¼š{t_collect - t0:.2f}, æ ·æœ¬æ•°ï¼š{len(actions)}, é‡‡æ ·é€Ÿåº¦ï¼š{len(actions)/ (t_collect - t0):.2f} æ ·æœ¬/s, è®¡ç®—ä¼˜åŠ¿æ—¶é—´ï¼š{t_sample - t_collect:.2f}, ç»Ÿè®¡æ—¶é—´ï¼š{t_stat - t_sample:.2f}, æ›´æ–°æ—¶é—´ï¼š{t_update - t_stat:.2f}, ä»·å€¼æŸå¤±ï¼š{value_loss:.6f}")
        finally:
            # ç»“æŸæ—¶ä¿å­˜æ¨¡å‹ï¼Œæ–‡ä»¶ååŒ…å«ç´¯è®¡è¿­ä»£æ¬¡æ•°ä¸batchå¤§å°
            try:
                curr_iter = self.start_iter + update_count
                final_path = self._format_save_path(save_path, curr_iter, steps_per_update)
                self.save_model(final_path)
            except Exception as e:
                print(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            
            pbar.close()
            if vec_env is not None:
                vec_env.close()
            
            print("=" * 70)
            print("è®­ç»ƒå®Œæˆ!")
            total_time = time.time() - start_time
            print(f"æ€»æ—¶é—´: {total_time:.2f}ç§’")
            print(f"æ€»æ­¥æ•°: {step_count}")
            print(f"æ€»æ›´æ–°æ¬¡æ•°: {update_count}")
            print(f"å¹³å‡æ¯æ¬¡æ›´æ–°è€—æ—¶: {total_time/update_count:.2f}ç§’")
            
            if len(episode_rewards) > 0:
                print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
            print("=" * 70)
        
        return episode_rewards
    
    def evaluate(self, num_episodes=10, render=False):
        """è¯„ä¼°ç­–ç•¥"""
        print(f"\nå¼€å§‹è¯„ä¼° {num_episodes} ä¸ªå›åˆ...")
        episode_rewards = []
        max_tiles = []
        
        for ep in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                if render:
                    self.env.render()
                
                action, _ = self.select_action(state)
                state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward
            
            max_tile = np.max(state)
            episode_rewards.append(episode_reward)
            max_tiles.append(max_tile)
            
            print(f"å›åˆ {ep+1}/{num_episodes}: å¥–åŠ± = {episode_reward:.2f}, æœ€å¤§æ–¹å— = {max_tile}")
        
        avg_reward = np.mean(episode_rewards)
        avg_max_tile = np.mean(max_tiles)
        
        print(f"\nè¯„ä¼°ç»“æœ:")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"å¹³å‡æœ€å¤§æ–¹å—: {avg_max_tile:.2f}")
        print(f"å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
        
        return episode_rewards, max_tiles
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_state_dict': self.policy.state_dict(),
            'value_state_dict': self.value_net.state_dict(),
        }, path)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.value_net.load_state_dict(checkpoint['value_state_dict'])
        # å°è¯•ä»æ–‡ä»¶åè§£æè¿­ä»£æ¬¡æ•°ï¼Œå¦‚ *_<iters>it_*.pth
        try:
            fname = os.path.basename(str(path))
            m = re.search(r"_(\d+)it(?:_|\.pth$)", fname)
            if m:
                self.start_iter = int(m.group(1))
                print(f"æ¨¡å‹å·²ä» {path} åŠ è½½ (èµ·å§‹è¿­ä»£={self.start_iter})")
                return
        except Exception:
            pass
        print(f"æ¨¡å‹å·²ä» {path} åŠ è½½")

    def _format_save_path(self, save_path, iters: int, batch_size: int) -> str:
        """æ ¹æ®ç»™å®šåŸºè·¯å¾„/æ–‡ä»¶åï¼Œç”Ÿæˆæºå¸¦è¿­ä»£ä¸batchä¿¡æ¯çš„ä¿å­˜è·¯å¾„ã€‚"""
        base_default = "trpo_game2048_model"
        ext = ".pth"
        if not save_path:
            directory = "."
            base = base_default
        else:
            directory, name = os.path.split(save_path)
            if not directory:
                directory = "."
            root, ext_in = os.path.splitext(name)
            if ext_in:
                base = root
            else:
                # ä¼ å…¥çš„æ˜¯ä¸å¸¦æ‰©å±•åçš„åŸºå
                base = name if name else base_default
        filename = f"{base}_{iters}it_{batch_size}batch{ext}"
        return os.path.join(directory, filename)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TRPOè®­ç»ƒç”¨äº2048æ¸¸æˆ')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--total-steps', type=int, default=100000,
                        help='æ€»è®­ç»ƒæ­¥æ•° (-1è¡¨ç¤ºæ— é™è®­ç»ƒ)')
    parser.add_argument('--steps-per-update', type=int, default=2048,
                        help='æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°')
    parser.add_argument('--num-envs', type=int, default=1,
                        help='å¹¶è¡Œç¯å¢ƒæ•°é‡(>1å¯ç”¨æ‰¹é‡é‡‡æ ·)')
    parser.add_argument('--vec-impl', type=str, choices=['auto', 'inproc', 'mp'], default='auto',
                        help='å‘é‡åŒ–ç¯å¢ƒå®ç°ï¼šauto(é»˜è®¤>1æ—¶ç”¨mp)ã€inproc(å•è¿›ç¨‹å¤šç¯å¢ƒ)ã€mp(å¤šè¿›ç¨‹)')
    parser.add_argument('--target', type=int, default=2048,
                        help='ç¯å¢ƒçš„ç›®æ ‡æ•°å€¼ï¼ˆä¾‹å¦‚ 2048/1024/4096ï¼‰')
    
    # ç½‘ç»œå‚æ•°
    parser.add_argument('--hidden-dim', type=int, default=256,
                        help='éšè—å±‚ç»´åº¦')
    
    # TRPOå‚æ•°
    parser.add_argument('--gamma', type=float, default=0.99,
                        help='æŠ˜æ‰£å› å­')
    parser.add_argument('--lam', type=float, default=0.95,
                        help='GAEå‚æ•°lambda')
    parser.add_argument('--max-kl', type=float, default=0.01,
                        help='æœ€å¤§KLæ•£åº¦çº¦æŸ')
    parser.add_argument('--damping', type=float, default=1e-2,
                        help='FisherçŸ©é˜µé˜»å°¼ç³»æ•°')
    parser.add_argument('--entropy-coeff', type=float, default=0.01,
                        help='ç†µæ­£åˆ™åŒ–ç³»æ•°')
    parser.add_argument('--value-lr', type=float, default=1e-3,
                        help='ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡')
    parser.add_argument('--train-value-iters', type=int, default=10,
                        help='ä»·å€¼ç½‘ç»œè®­ç»ƒè¿­ä»£æ¬¡æ•°')
    
    # æ¨¡å‹ä¿å­˜/åŠ è½½
    parser.add_argument('--save-path', type=str, default='trpo_game2048_model.pth',
                        help='æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--load-path', type=str, default=None,
                        help='åŠ è½½é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--eval-episodes', type=int, default=10,
                        help='è¯„ä¼°å›åˆæ•°')
    parser.add_argument('--no-eval', action='store_true',
                        help='è®­ç»ƒåä¸è¿›è¡Œè¯„ä¼°')
    
    # è®¾å¤‡é€‰æ‹©ï¼šå…è®¸ 'auto', 'cpu', 'cuda' æˆ– 'cuda:<gpu_id>'
    def device_type(s):
        s = s.strip()
        if s in ('auto', 'cpu', 'cuda'):
            return s
        if s.startswith('cuda:'):
            try:
                idx = int(s.split(':', 1)[1])
                if idx < 0:
                    raise ValueError
                return f'cuda:{idx}'
            except Exception:
                raise argparse.ArgumentTypeError(
                    "--device must be 'auto', 'cpu', 'cuda' or 'cuda:<non-negative-int>'"
                )
        raise argparse.ArgumentTypeError(
            "--device must be 'auto', 'cpu', 'cuda' or 'cuda:<non-negative-int>'"
        )

    parser.add_argument('--device', type=device_type, default='auto',
                        help="è®­ç»ƒè®¾å¤‡ã€‚å¯ç”¨: 'auto', 'cpu', 'cuda', æˆ– 'cuda:<gpu_id>'")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    # æ‰“å°é…ç½®
    print("\n" + "ğŸ®" * 35)
    print("TRPO 2048æ¸¸æˆè®­ç»ƒ")
    print("ğŸ®" * 35)
    print("\né…ç½®å‚æ•°:")
    print(f"  è®­ç»ƒæ­¥æ•°: {'âˆ (æ— é™)' if args.total_steps == -1 else args.total_steps}")
    print(f"  æ¯æ¬¡æ›´æ–°æ­¥æ•°: {args.steps_per_update}")
    print(f"  éšè—å±‚ç»´åº¦: {args.hidden_dim}")
    print(f"  æœ€å¤§KLæ•£åº¦: {args.max_kl}")
    print(f"  é˜»å°¼ç³»æ•°: {args.damping}")
    print(f"  å¹¶è¡Œç¯å¢ƒæ•°: {args.num_envs}")
    print(f"  å‘é‡åŒ–å®ç°: {args.vec_impl}")
    print(f"  è®¾å¤‡: {device}")
    print(f"  ç›®æ ‡æ•°å€¼: {args.target}")
    print(f"  æ¨¡å‹ä¿å­˜è·¯å¾„: {args.save_path}")
    if args.load_path:
        print(f"  åŠ è½½æ¨¡å‹: {args.load_path}")
    print()
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make("Game2048-v0", target=args.target, debug=True)
    
    # åˆ›å»ºTRPOæ™ºèƒ½ä½“
    agent = TRPO(
        env=env,
        hidden_dim=args.hidden_dim,
        gamma=args.gamma,
        lam=args.lam,
        max_kl=args.max_kl,
        damping=args.damping,
        entropy_coeff=args.entropy_coeff,
        value_lr=args.value_lr,
        train_value_iters=args.train_value_iters,
        device=device,
        num_envs=args.num_envs,
        vec_impl=args.vec_impl,
        env_target=args.target,
    )
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.load_path:
        agent.load_model(args.load_path)
    
    # è®­ç»ƒ
    agent.train(total_steps=args.total_steps, steps_per_update=args.steps_per_update, save_path=args.save_path)
    
    # è®­ç»ƒé˜¶æ®µå·²æ ¹æ®è¿›åº¦è‡ªåŠ¨ä¿å­˜ï¼ˆæ–‡ä»¶ååŒ…å« it ä¸ batch ä¿¡æ¯ï¼‰ï¼Œæ­¤å¤„æ— éœ€é‡å¤ä¿å­˜
    
    # è¯„ä¼°
    if not args.no_eval:
        agent.evaluate(num_episodes=args.eval_episodes, render=False)
    
    # å…³é—­ç¯å¢ƒ
    env.close()


if __name__ == "__main__":
    main()
