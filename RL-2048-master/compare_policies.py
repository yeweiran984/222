"""
å¯¹æ¯”æµ‹è¯•: éšæœºç­–ç•¥ vs è®­ç»ƒçš„TRPOæ™ºèƒ½ä½“
"""

import numpy as np
import torch
import gymnasium as gym
import game2048
from trpo_game2048_simple import SimpleTRPO
from trpo_game2048 import TRPO
import time
import argparse


def test_random_policy(env, num_episodes=10):
    """æµ‹è¯•éšæœºç­–ç•¥"""
    print("=" * 70)
    print("æµ‹è¯•éšæœºç­–ç•¥ (å®Œå…¨éšæœºé€‰æ‹©åŠ¨ä½œ)")
    print("=" * 70)
    
    episode_rewards = []
    max_tiles = []
    steps_list = []
    
    per_episode = []  # collect for sorting by reward desc
    for ep in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0.0
        done = False
        steps = 0
        
        while not done:
            action = env.action_space.sample()  # éšæœºåŠ¨ä½œ
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            episode_reward += float(reward)
            steps += 1
        
        max_tile = np.max(state)
        episode_rewards.append(episode_reward)
        max_tiles.append(max_tile)
        steps_list.append(steps)
        per_episode.append((episode_reward, int(max_tile), steps, ep + 1))
    
    # æŒ‰å¥–åŠ±é™åºæ‰“å°
    print("\næŒ‰å¥–åŠ±é™åº (éšæœºç­–ç•¥):")
    for rank, (r, tile, st, original_ep) in enumerate(sorted(per_episode, key=lambda x: x[0], reverse=True), 1):
        print(f"#{rank:2d} å›åˆ(åŸ{original_ep:2d}): å¥–åŠ±={r:8.1f}, æœ€å¤§æ–¹å—={tile:4d}, æ­¥æ•°={st:4d}")
    
    print(f"\néšæœºç­–ç•¥ç»Ÿè®¡:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):8.1f} (Â± {np.std(episode_rewards):.1f})")
    print(f"  å¹³å‡æœ€å¤§æ–¹å—: {np.mean(max_tiles):6.1f} (Â± {np.std(max_tiles):.1f})")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(steps_list):6.1f} (Â± {np.std(steps_list):.1f})")
    
    return episode_rewards, max_tiles, steps_list


def test_trpo_policy(env, model_path, num_episodes=10):
    """æµ‹è¯•TRPOç­–ç•¥"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•TRPOè®­ç»ƒçš„ç­–ç•¥")
    print("=" * 70)
    
    # åŠ è½½æ™ºèƒ½ä½“
    # env = gym.make("Game2048-v0", debug=True)
    agent = TRPO(env)
    try:
        agent.load_model(model_path)
        print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}\n")
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•åŠ è½½æ¨¡å‹ {model_path}")
        raise e
    
    episode_rewards = []
    max_tiles = []
    steps_list = []
    
    per_episode = []  # collect for sorting by reward desc
    for ep in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0.0
        done = False
        steps = 0
        
        while not done:
            action, _ = agent.select_action(state)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            episode_reward += float(reward)
            steps += 1
        
        max_tile = np.max(state)
        episode_rewards.append(episode_reward)
        max_tiles.append(max_tile)
        steps_list.append(steps)
        per_episode.append((episode_reward, int(max_tile), steps, ep + 1))
    
    # æŒ‰å¥–åŠ±é™åºæ‰“å°
    print("\næŒ‰å¥–åŠ±é™åº (TRPOç­–ç•¥):")
    for rank, (r, tile, st, original_ep) in enumerate(sorted(per_episode, key=lambda x: x[0], reverse=True), 1):
        print(f"#{rank:2d} å›åˆ(åŸ{original_ep:2d}): å¥–åŠ±={r:8.1f}, æœ€å¤§æ–¹å—={tile:4d}, æ­¥æ•°={st:4d}")
    
    print(f"\nTRPOç­–ç•¥ç»Ÿè®¡:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):8.1f} (Â± {np.std(episode_rewards):.1f})")
    print(f"  å¹³å‡æœ€å¤§æ–¹å—: {np.mean(max_tiles):6.1f} (Â± {np.std(max_tiles):.1f})")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(steps_list):6.1f} (Â± {np.std(steps_list):.1f})")
    
    return episode_rewards, max_tiles, steps_list


def compare_policies(num_episodes=20, model_path="trpo_game2048_simple.pth", debug: bool = False):
    """å¯¹æ¯”ä¸¤ç§ç­–ç•¥"""
    print("\n" + "ğŸ®" * 35)
    print("2048æ¸¸æˆç­–ç•¥å¯¹æ¯”æµ‹è¯•")
    print("ğŸ®" * 35 + "\n")
    
    # åˆ›å»ºç¯å¢ƒ
    env_random = gym.make("Game2048-v0")  # éšæœºç­–ç•¥ç¯å¢ƒä¸ä¼  debug
    
    # æµ‹è¯•éšæœºç­–ç•¥
    random_rewards, random_tiles, random_steps = test_random_policy(env_random, num_episodes)
    
    # æµ‹è¯•TRPOç­–ç•¥
    env_agent = gym.make("Game2048-v0", debug=debug)  # ä»…ä¼ ç»™æ™ºèƒ½ä½“çš„ç¯å¢ƒ
    trpo_rewards, trpo_tiles, trpo_steps = test_trpo_policy(
        env_agent, model_path, num_episodes
    )
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¹æ¯”ç»“æœ")
    print("=" * 70)
    
    print("\nå¹³å‡å¥–åŠ±å¯¹æ¯”:")
    print(f"  éšæœºç­–ç•¥: {np.mean(random_rewards):8.1f}")
    print(f"  TRPOç­–ç•¥:  {np.mean(trpo_rewards):8.1f}")
    improvement_reward = (np.mean(trpo_rewards) - np.mean(random_rewards)) / np.mean(random_rewards) * 100
    print(f"  æå‡:      {improvement_reward:7.1f}% {'âœ…' if improvement_reward > 0 else 'âŒ'}")
    
    print("\nå¹³å‡æœ€å¤§æ–¹å—å¯¹æ¯”:")
    print(f"  éšæœºç­–ç•¥: {np.mean(random_tiles):6.1f}")
    print(f"  TRPOç­–ç•¥:  {np.mean(trpo_tiles):6.1f}")
    improvement_tile = (np.mean(trpo_tiles) - np.mean(random_tiles)) / np.mean(random_tiles) * 100
    print(f"  æå‡:      {improvement_tile:6.1f}% {'âœ…' if improvement_tile > 0 else 'âŒ'}")
    
    print("\nå¹³å‡æ­¥æ•°å¯¹æ¯”:")
    print(f"  éšæœºç­–ç•¥: {np.mean(random_steps):6.1f}")
    print(f"  TRPOç­–ç•¥:  {np.mean(trpo_steps):6.1f}")
    improvement_steps = (np.mean(trpo_steps) - np.mean(random_steps)) / np.mean(random_steps) * 100
    print(f"  æå‡:      {improvement_steps:6.1f}% {'âœ…' if improvement_steps > 0 else 'âŒ'}")
    
    # æ–¹å—åˆ†å¸ƒå¯¹æ¯”
    print("\næœ€å¤§æ–¹å—åˆ†å¸ƒå¯¹æ¯”:")
    print("-" * 70)
    print(f"{'æ–¹å—':^10s} | {'éšæœºç­–ç•¥':^15s} | {'TRPOç­–ç•¥':^15s}")
    print("-" * 70)
    
    all_tiles = sorted(set(list(random_tiles) + list(trpo_tiles)), reverse=True)
    for tile in all_tiles:
        random_count = random_tiles.count(tile)
        trpo_count = trpo_tiles.count(tile)
        random_pct = random_count / len(random_tiles) * 100
        trpo_pct = trpo_count / len(trpo_tiles) * 100
        print(f"{int(tile):^10d} | {random_count:3d} ({random_pct:5.1f}%) | {trpo_count:3d} ({trpo_pct:5.1f}%)")
    
    print("-" * 70)
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ’¡ ç»“è®º")
    print("=" * 70)
    
    if improvement_reward > 50:
        print("âœ… TRPOæ™ºèƒ½ä½“è¡¨ç°ä¼˜å¼‚,æ˜¾è‘—è¶…è¿‡éšæœºç­–ç•¥!")
    elif improvement_reward > 0:
        print("âœ… TRPOæ™ºèƒ½ä½“æœ‰æ‰€æ”¹è¿›,ä½†è¿˜æœ‰æå‡ç©ºé—´")
        print("ğŸ’¡ å»ºè®®: ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´è¶…å‚æ•°")
    else:
        print("âŒ TRPOæ™ºèƒ½ä½“æœªèƒ½è¶…è¿‡éšæœºç­–ç•¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹,å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒ")
    
    print("\nå¦‚æœç»“æœä¸ç†æƒ³,å¯ä»¥:")
    print("  1. å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•° (num_iterations)")
    print("  2. è°ƒæ•´å­¦ä¹ ç‡å’Œå…¶ä»–è¶…å‚æ•°")
    print("  3. ä½¿ç”¨æ›´å¤§çš„ç½‘ç»œ (hidden_dim)")
    print("  4. æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡")
    
    print("\n" + "=" * 70)
    
    env_random.close()
    env_agent.close()


def quick_compare(debug: bool = False):
    """å¿«é€Ÿå¯¹æ¯” (å°‘é‡å›åˆ)"""
    print("\nå¿«é€Ÿå¯¹æ¯”æµ‹è¯• (5ä¸ªå›åˆ)\n")
    compare_policies(num_episodes=5, debug=debug)


def full_compare(model_path="trpo_game2048_simple.pth", debug: bool = False):
    """å®Œæ•´å¯¹æ¯” (æ›´å¤šå›åˆ,æ›´å‡†ç¡®)"""
    print("\nå®Œæ•´å¯¹æ¯”æµ‹è¯• (20ä¸ªå›åˆ)\n")
    compare_policies(num_episodes=20, model_path=model_path, debug=debug)


if __name__ == "__main__":
    args = argparse.ArgumentParser(description="å¯¹æ¯”æµ‹è¯•: éšæœºç­–ç•¥ vs è®­ç»ƒçš„TRPOæ™ºèƒ½ä½“")
    args.add_argument("--path", type=str, default="trpo_game2048_simple.pth", help="TRPOæ¨¡å‹æ–‡ä»¶è·¯å¾„")
    args.add_argument("--debug", action="store_true", help="ä»…ä¼ ç»™TRPOæ™ºèƒ½ä½“ç¯å¢ƒçš„debugæ ‡å¿—")
    parsed_args = args.parse_args()
    full_compare(model_path=parsed_args.path, debug=parsed_args.debug)
